import { NextRequest, NextResponse } from "next/server";
import { getServerSession } from "next-auth";
import { authOptions } from "@/lib/auth";
import { 
  ensureConversation, 
  getHistory, 
  appendMessage, 
  conversationManager 
} from "@/lib/conversation-manager";
import { streamOpenAI } from "@/lib/providers/openai";
import { streamGPT5 } from "@/lib/providers/gpt5";
import { streamGemini } from "@/lib/providers/gemini";
import { streamPerplexity } from "@/lib/providers/perplexity";
import { streamGrok } from "@/lib/providers/grok";
import { randomUUID } from "crypto";

export const runtime = "nodejs";

type Body = {
  provider: "openai" | "gpt5" | "gemini" | "perplexity" | "grok";
  model: string;
  input: string;
  system?: string;
  module?: string;
  conversationId?: string;
};

export async function POST(req: NextRequest) {
  const startTime = Date.now();
  
  try {
    const session = await getServerSession(authOptions);
    const userId = session?.user?.id || randomUUID();
    
    const body = (await req.json()) as Body;
    const { provider, model, input, system, module = "chat", conversationId } = body;

    console.log(`ğŸš€ [CHAT-STREAM] START - Provider: ${provider}, Model: ${model}, User: ${userId}`);

    if (!input?.trim()) {
      return NextResponse.json({ error: "Input Ã© obrigatÃ³rio" }, { status: 400 });
    }

    // 1) Garantir conversa
    const conv = await ensureConversation(conversationId, userId, module);
    const finalConversationId = conv.id;

    // 2) Recuperar histÃ³rico com contexto inteligente
    const history = await conversationManager.getConversationHistory(finalConversationId, userId);
    const intelligentContext = conversationManager.generateIntelligentContext(history, input);
    console.log(`ğŸ“š [CHAT-STREAM] History loaded: ${history.length} messages, intelligent context: ${intelligentContext.length} messages`);

    // 3) Adicionar mensagem do usuÃ¡rio
    await appendMessage(finalConversationId, "user", input, provider, model);

    // 4) Criar system prompt contextual baseado no histÃ³rico
    const contextualSystemPrompt = createContextualSystemPrompt(intelligentContext, system, module);
    
    // 5) Criar stream baseado no provedor
    let stream: any;
    let finalProvider = provider;
    let finalModel = model;
    
    console.log(`ğŸ”§ [CHAT-STREAM] Creating stream for provider: ${provider}, model: ${model}`);
    
    switch (provider) {
      case "openai":
        stream = await streamOpenAI(model, intelligentContext, input, contextualSystemPrompt);
        console.log(`âœ… [CHAT-STREAM] OpenAI stream created successfully`);
        break;
      case "gpt5":
        stream = await streamGPT5(model, intelligentContext, input, contextualSystemPrompt);
        console.log(`âœ… [CHAT-STREAM] GPT5 stream created successfully`);
        break;
      case "gemini":
        stream = await streamGemini(model, intelligentContext, input, contextualSystemPrompt);
        console.log(`âœ… [CHAT-STREAM] Gemini stream created successfully`);
        break;
      case "perplexity":
        stream = await streamPerplexity(model, intelligentContext, input, contextualSystemPrompt);
        console.log(`âœ… [CHAT-STREAM] Perplexity stream created successfully`);
        break;
      case "grok":
        try {
          stream = await streamGrok(model, intelligentContext, input, contextualSystemPrompt);
          console.log(`âœ… [CHAT-STREAM] Grok stream created successfully`);
        } catch (grokError) {
          console.error(`âŒ [CHAT-STREAM] Grok failed, falling back to Gemini:`, grokError);
          // Fallback to Gemini if Grok fails
          stream = await streamGemini("gemini-2.0-flash-exp", intelligentContext, input, contextualSystemPrompt);
          console.log(`âœ… [CHAT-STREAM] Gemini fallback stream created successfully`);
          // Update provider for metadata
          finalProvider = "gemini";
          finalModel = "gemini-2.0-flash-exp";
        }
        break;
      default:
        return NextResponse.json({ error: "Provider invÃ¡lido" }, { status: 400 });
    }

    // 5) Criar ReadableStream para resposta
    const encoder = new TextEncoder();
    let fullResponse = "";
    let streamCompleted = false;

    console.log(`ğŸ”§ [CHAT-STREAM] Creating ReadableStream for response...`);

    const readableStream = new ReadableStream({
      async start(controller) {
        try {
          console.log(`ğŸš€ [CHAT-STREAM] ReadableStream started`);
          
          // Enviar metadados iniciais
          const metadata = {
            type: "metadata",
            conversationId: finalConversationId,
            provider: finalProvider,
            model: finalModel
          };
          controller.enqueue(encoder.encode(`data: ${JSON.stringify(metadata)}\n\n`));
          console.log(`ğŸ“¤ [CHAT-STREAM] Metadata sent:`, metadata);

          // Processar stream baseado no provedor
          if (finalProvider === "openai" || finalProvider === "gpt5") {
            console.log(`ğŸ“¡ [CHAT-STREAM] Processing OpenAI/GPT5 stream...`);
            let chunkCount = 0;
            for await (const chunk of stream) {
              chunkCount++;
              const content = chunk.choices[0]?.delta?.content;
              if (content) {
                fullResponse += content;
                const data = {
                  type: "content",
                  content
                };
                controller.enqueue(encoder.encode(`data: ${JSON.stringify(data)}\n\n`));
                // Log removido para reduzir ruÃ­do no terminal
              }
            }
            console.log(`âœ… [CHAT-STREAM] OpenAI/GPT5 stream completed. Total chunks: ${chunkCount}`);
          } else if (finalProvider === "gemini") {
            for await (const chunk of stream.stream) {
              const text = chunk.text();
              if (text) {
                fullResponse += text;
                const data = {
                  type: "content",
                  content: text
                };
                controller.enqueue(encoder.encode(`data: ${JSON.stringify(data)}\n\n`));
              }
            }
          } else if (finalProvider === "perplexity") {
            for await (const chunk of stream) {
              const content = chunk.choices[0]?.delta?.content;
              if (content) {
                fullResponse += content;
                const data = {
                  type: "content",
                  content
                };
                controller.enqueue(encoder.encode(`data: ${JSON.stringify(data)}\n\n`));
              }
            }
          } else if (finalProvider === "grok") {
            for await (const chunk of stream) {
              const content = chunk.choices[0]?.delta?.content;
              if (content) {
                fullResponse += content;
                const data = {
                  type: "content",
                  content
                };
                controller.enqueue(encoder.encode(`data: ${JSON.stringify(data)}\n\n`));
              }
            }
          }

          // Enviar metadados finais
          const finalMetadata = {
            metadata: {
              model: finalModel,
              provider: finalProvider,
              tier: finalProvider === 'gemini' ? 'IA_ECO' : 'IA',
              complexity: 'simple',
              module: module,
              tokens: 0
            },
            meta: {
              provider: finalProvider,
              model: finalModel,
              timestamp: Date.now()
            }
          };
          controller.enqueue(encoder.encode(`data: ${JSON.stringify(finalMetadata)}\n\n`));
          console.log(`ğŸ“Š [CHAT-STREAM] Final metadata sent:`, finalMetadata);

          // Enviar sinal de fim
          console.log(`ğŸ [CHAT-STREAM] Sending done signal...`);
          controller.enqueue(encoder.encode(`data: {"type": "done"}\n\n`));
          controller.close();
          console.log(`âœ… [CHAT-STREAM] Stream closed successfully`);
          
          // Marcar stream como completo e salvar resposta
          streamCompleted = true;
          try {
            await appendMessage(
              finalConversationId, 
              "assistant", 
              fullResponse, 
              finalProvider, 
              finalModel
            );
            
            console.log(`âœ… [CHAT-STREAM] Response saved: ${fullResponse.length} chars`);
          } catch (error) {
            console.error("âŒ [CHAT-STREAM] Error saving response:", error);
          }

        } catch (error) {
          console.error("âŒ [CHAT-STREAM] Stream error:", error);
          controller.error(error);
        }
      }
    });

    console.log(`ğŸ“¤ [CHAT-STREAM] Returning stream response with headers...`);
    
    return new Response(readableStream, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    });

  } catch (err: any) {
    console.error("âŒ [CHAT-STREAM] ERROR:", err);
    return NextResponse.json({ 
      error: err?.message ?? "Erro interno do servidor" 
    }, { status: 500 });
  }
}

/**
 * Cria um system prompt contextual baseado no histÃ³rico da conversa
 * para evitar introduÃ§Ãµes desnecessÃ¡rias e manter continuidade
 */
function createContextualSystemPrompt(
  history: any[], 
  customSystem?: string, 
  module: string = "chat"
): string {
  // Se hÃ¡ histÃ³rico, criar prompt contextual para QUALQUER tema
  if (history && history.length > 0) {
    const lastUserMessage = history.filter(m => m.role === 'user').pop();
    const lastAssistantMessage = history.filter(m => m.role === 'assistant').pop();
    
    // Detectar se Ã© continuaÃ§Ã£o de qualquer conversa (nÃ£o apenas matemÃ¡tica)
    const hasHistory = history.length > 1;
    const isContinuation = hasHistory && (
      lastUserMessage?.content || 
      lastAssistantMessage?.content
    );
    
    if (isContinuation) {
      return `VocÃª Ã© um assistente educacional brasileiro.

ğŸš¨ IDIOMA OBRIGATÃ“RIO: Responda EXCLUSIVAMENTE em PortuguÃªs Brasileiro (PT-BR).

CONTEXTO DA CONVERSA:
- Esta Ã© uma CONTINUAÃ‡ÃƒO de uma conversa existente
- O usuÃ¡rio jÃ¡ estÃ¡ familiarizado com o tÃ³pico atual
- NÃƒO faÃ§a introduÃ§Ãµes longas ou repetitivas
- Seja DIRETO e FOQUE na resposta especÃ­fica

INSTRUÃ‡Ã•ES CRÃTICAS PARA CONTINUIDADE:
- NÃƒO comece com "Oi! Que legal vocÃª estar interessado..."
- NÃƒO faÃ§a introduÃ§Ãµes sobre "o que Ã©" se jÃ¡ foi explicado
- NÃƒO repita informaÃ§Ãµes jÃ¡ dadas na conversa
- Seja CONCISO e DIRETO
- Responda APENAS o que foi perguntado
- Use sÃ­mbolos Unicode: xÂ², âˆš, Â±, Ã·, Ã—, Â½, Ï€
- NUNCA use LaTeX: $...$, $$...$$, \\frac, etc.

Se o usuÃ¡rio pedir algo especÃ­fico (fÃ³rmulas, explicaÃ§Ãµes, exemplos), dÃª diretamente sem explicaÃ§Ãµes longas.`;
    }
  }
  
  // Prompt padrÃ£o para novas conversas
  return customSystem || `VocÃª Ã© um assistente educacional brasileiro.

ğŸš¨ IDIOMA OBRIGATÃ“RIO: Responda EXCLUSIVAMENTE em PortuguÃªs Brasileiro (PT-BR).

INSTRUÃ‡Ã•ES:
- Seja amigÃ¡vel mas DIRETO
- Evite introduÃ§Ãµes muito longas
- Foque na resposta especÃ­fica
- Use sÃ­mbolos Unicode: xÂ², âˆš, Â±, Ã·, Ã—, Â½, Ï€
- NUNCA use LaTeX: $...$, $$...$$, \\frac, etc.

Contexto: MÃ³dulo ${module}`;
}
