import { NextRequest, NextResponse } from 'next/server'
import { getServerSession } from 'next-auth'
import { authOptions } from '@/lib/auth'
import { openai, selectModel, getModelConfig, MODELS } from '@/lib/openai'

export async function POST(request: NextRequest) {
  try {
    // Verificar autentica√ß√£o usando NextAuth (desabilitado temporariamente para desenvolvimento)
    const session = await getServerSession(authOptions)
    // if (!session) {
    //   return NextResponse.json({ error: 'Unauthorized' }, { status: 401 })
    // }

    const { message, module, conversationId, history = [] } = await request.json()

    if (!message) {
      return NextResponse.json({ error: 'Message is required' }, { status: 400 })
    }

    // Automatic model selection using classifier
    let selectedModel = 'gpt-4o-mini'; // Default fallback
    
    try {
      console.log('üîç Classifying message for model selection...');
      const classifyResponse = await fetch(`${request.nextUrl.origin}/api/router/classify`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ message }),
      });
      
      if (classifyResponse.ok) {
        const { classification } = await classifyResponse.json();
        selectedModel = classification === 'complexa' ? 'gpt-5-chat-latest' : 'gpt-4o-mini';
        console.log(`‚úÖ Model selected: ${selectedModel} (classification: ${classification})`);
      } else {
        console.warn('‚ö†Ô∏è Classifier failed, using default model');
      }
    } catch (classifyError) {
      console.error('‚ùå Classification error:', classifyError);
      // Continue with default model
    }

    // System prompts baseados no m√≥dulo classificado
    const systemPrompts = {
      professor: `Voc√™ √© um assistente educacional especializado em pedagogia. Forne√ßa explica√ß√µes claras, did√°ticas e adequadas para estudantes brasileiros. Sempre use exemplos pr√°ticos e linguagem acess√≠vel.

IMPORTANTE: Para express√µes matem√°ticas, sempre use s√≠mbolos Unicode em vez de LaTeX. Por exemplo:
- Use ‚àö para raiz quadrada (n√£o \\sqrt)
- Use ‚ÅÑ para fra√ß√µes (n√£o \\frac)
- Use ¬± para mais/menos (n√£o \\pm)
- Use √ó para multiplica√ß√£o (n√£o \\times)
- Use √∑ para divis√£o (n√£o \\div)
- Use ‚â§ para menor ou igual (n√£o \\leq)
- Use ‚â• para maior ou igual (n√£o \\geq)
- Use œÄ para pi (n√£o \\pi)
- Use Œ±, Œ≤, Œ≥, Œ¥, etc. para letras gregas (n√£o \\alpha, \\beta, etc.)`,
      
      ti: `Voc√™ √© um especialista em suporte t√©cnico para escolas. Ajude com problemas de tecnologia, equipamentos, sistemas e conectividade. Seja pr√°tico e ofere√ßa solu√ß√µes passo a passo.`,
      
      secretaria: `Voc√™ √© um assistente da secretaria escolar. Ajude com documentos de alunos, matr√≠culas, declara√ß√µes e procedimentos administrativos acad√™micos.`,
      
      financeiro: `Voc√™ √© um assistente financeiro escolar. Ajude com quest√µes de pagamentos, mensalidades, boletos e quest√µes financeiras de alunos e fam√≠lias.`,
      
      rh: `Voc√™ √© um assistente de recursos humanos escolar. Ajude funcion√°rios com quest√µes trabalhistas, benef√≠cios, f√©rias, treinamentos e pol√≠ticas internas.`,
      
      coordenacao: `Voc√™ √© um assistente de coordena√ß√£o pedag√≥gica. Ajude com gest√£o pedag√≥gica, calend√°rio escolar e coordena√ß√£o acad√™mica.`,
      
      bem_estar: `Voc√™ √© um assistente de bem-estar escolar. Ofere√ßa apoio emocional, ajude com ansiedade, conflitos e quest√µes de sa√∫de mental da comunidade escolar.`,
      
      social_media: `Voc√™ √© um especialista em marketing digital escolar. Ajude a criar posts, destacar conquistas, celebrar resultados e desenvolver conte√∫do para redes sociais.`,
      
      atendimento: `Voc√™ √© um assistente de atendimento geral escolar. Ajude com d√∫vidas gerais, informa√ß√µes e primeiro contato. Seja cordial e direcione para o setor correto quando necess√°rio.`,
    }

    const systemPrompt = systemPrompts[module as keyof typeof systemPrompts] || systemPrompts.atendimento

    // Preparar mensagens para OpenAI com hist√≥rico
    const messages = [
      { role: 'system', content: systemPrompt },
      ...history.slice(-10), // Include last 10 messages for context
      { role: 'user', content: message }
    ]

    // Get model configuration
    const modelConfig = getModelConfig(selectedModel)
    
    console.log(`Using model: ${selectedModel} for module: ${module}`)
    
    // Configurar streaming
    const stream = await openai.chat.completions.create({
      model: selectedModel,
      messages: messages as any,
      stream: modelConfig.stream,
      temperature: modelConfig.temperature,
      max_tokens: modelConfig.max_tokens,
    })

    // Criar ReadableStream para streaming
    const encoder = new TextEncoder()
    
    const readableStream = new ReadableStream({
      async start(controller) {
        let totalTokens = 0
        let isClosed = false
        
        const safeEnqueue = (data: string) => {
          if (!isClosed) {
            try {
              controller.enqueue(encoder.encode(data))
            } catch (error) {
              console.warn('Controller already closed, skipping enqueue')
              isClosed = true
            }
          }
        }
        
        const safeClose = () => {
          if (!isClosed) {
            try {
              controller.close()
              isClosed = true
            } catch (error) {
              console.warn('Controller already closed')
            }
          }
        }
        
        try {
          for await (const chunk of stream as any) {
            const content = chunk.choices[0]?.delta?.content || ''
            
            if (content) {
              // Enviar chunk de conte√∫do
              const data = JSON.stringify({ content })
              safeEnqueue(`data: ${data}\n\n`)
            }
            
            // Capturar uso de tokens
            if (chunk.usage) {
              totalTokens = chunk.usage.total_tokens || 0
            }
          }
          
          // Enviar metadados finais
          const metadata = JSON.stringify({ 
            metadata: { 
              tokens: totalTokens,
              model: selectedModel,
              tier: selectedModel === MODELS.COMPLEX ? 'IA_SUPER' : 'IA'
            }
          })
          safeEnqueue(`data: ${metadata}\n\n`)
          
          // Sinalizar fim do stream
          safeEnqueue('data: [DONE]\n\n')
          safeClose()
          
        } catch (error) {
          console.error('Streaming error:', error)
          const errorData = JSON.stringify({ error: 'Erro durante o streaming' })
          safeEnqueue(`data: ${errorData}\n\n`)
          safeClose()
        }
      }
    })

    return new Response(readableStream, {
      headers: {
        'Content-Type': 'text/event-stream; charset=utf-8',
        'Cache-Control': 'no-cache, no-transform',
        'Connection': 'keep-alive',
        'X-Accel-Buffering': 'no',
      },
    })

  } catch (error: any) {
    console.error('Chat API error:', error)
    
    // Preparar erro amig√°vel
    let friendlyError = 'Erro interno do servidor. Tente novamente.'
    
    if (error.message?.includes('rate limit')) {
      friendlyError = 'Limite de taxa excedido. Tente novamente em alguns minutos.'
    } else if (error.message?.includes('quota')) {
      friendlyError = 'Cota de tokens excedida. Verifique seu plano.'
    } else if (error.message?.includes('network')) {
      friendlyError = 'Erro de conex√£o. Verifique sua internet e tente novamente.'
    }

    return NextResponse.json({ error: friendlyError }, { status: 500 })
  }
}
