# üé§ Aplica√ß√£o da Estrutura Live-Audio nas Aulas - Plano de Implementa√ß√£o

## üìã An√°lise da Implementa√ß√£o Atual

### **Sistema TTS Atual nas Aulas**
- **Componente**: `BufferTTSPlayer` em `AnimationSlide`
- **Funcionamento**: Gera √°udio completo ‚Üí Buffer ‚Üí Reproduz
- **Lat√™ncia**: ~2-5 segundos para gera√ß√£o + reprodu√ß√£o
- **Qualidade**: Boa, mas n√£o em tempo real
- **API**: Usa endpoints REST com fallback

### **Limita√ß√µes Identificadas**
1. **Lat√™ncia alta**: Usu√°rio espera gera√ß√£o completa do √°udio
2. **Sem streaming**: N√£o h√° reprodu√ß√£o durante gera√ß√£o
3. **Sem visualiza√ß√£o**: N√£o h√° feedback visual do √°udio
4. **Arquitetura tradicional**: REST API em vez de WebSocket direto

## üöÄ Plano de Migra√ß√£o para Streaming Nativo

### **Fase 1: Componente de Streaming Baseado na Live-Audio**

#### 1.1 Criar `LiveAudioStreamPlayer`
```typescript
// components/audio/LiveAudioStreamPlayer.tsx
interface LiveAudioStreamPlayerProps {
  text: string;
  voice?: string;
  autoPlay?: boolean;
  showVisualization?: boolean;
  onAudioStart?: () => void;
  onAudioEnd?: () => void;
  onError?: (error: string) => void;
}

export default function LiveAudioStreamPlayer({
  text,
  voice = 'Orus',
  autoPlay = false,
  showVisualization = true,
  onAudioStart,
  onAudioEnd,
  onError
}: LiveAudioStreamPlayerProps) {
  // Implementa√ß√£o baseada na live-audio
  const [isConnected, setIsConnected] = useState(false);
  const [isStreaming, setIsStreaming] = useState(false);
  const [audioContext, setAudioContext] = useState<AudioContext | null>(null);
  const [session, setSession] = useState<Session | null>(null);
  
  // Conex√£o direta com Gemini Live API
  const connectToGemini = useCallback(async () => {
    const client = new GoogleGenAI({ apiKey: process.env.NEXT_PUBLIC_GEMINI_API_KEY });
    const session = await client.live.connect({
      model: 'gemini-2.5-flash-preview-native-audio-dialog',
      callbacks: {
        onopen: () => setIsConnected(true),
        onmessage: handleAudioResponse,
        onerror: handleError,
        onclose: () => setIsConnected(false),
      },
      config: {
        responseModalities: [Modality.AUDIO],
        speechConfig: {
          voiceConfig: { prebuiltVoiceConfig: { voiceName: voice } },
        },
      },
    });
    setSession(session);
  }, [voice]);

  // Streaming de texto para √°udio
  const streamTextToAudio = useCallback(async () => {
    if (!session || !text) return;
    
    setIsStreaming(true);
    onAudioStart?.();
    
    // Enviar texto para Gemini Live API
    await session.sendRealtimeInput({
      text: text
    });
  }, [session, text, onAudioStart]);

  return (
    <div className="live-audio-stream-player">
      {/* Controles de conex√£o e streaming */}
      {/* Visualiza√ß√£o de √°udio em tempo real */}
      {/* Status e progresso */}
    </div>
  );
}
```

#### 1.2 Implementar Visualiza√ß√£o de √Åudio
```typescript
// components/audio/AudioVisualizer.tsx
interface AudioVisualizerProps {
  audioContext: AudioContext;
  inputNode: AudioNode;
  outputNode: AudioNode;
}

export default function AudioVisualizer({ audioContext, inputNode, outputNode }: AudioVisualizerProps) {
  const [inputAnalyser, setInputAnalyser] = useState<AnalyserNode | null>(null);
  const [outputAnalyser, setOutputAnalyser] = useState<AnalyserNode | null>(null);
  const canvasRef = useRef<HTMLCanvasElement>(null);

  useEffect(() => {
    if (audioContext && inputNode && outputNode) {
      // Criar analisadores de frequ√™ncia
      const inputAnalyser = audioContext.createAnalyser();
      const outputAnalyser = audioContext.createAnalyser();
      
      inputAnalyser.fftSize = 256;
      outputAnalyser.fftSize = 256;
      
      inputNode.connect(inputAnalyser);
      outputNode.connect(outputAnalyser);
      
      setInputAnalyser(inputAnalyser);
      setOutputAnalyser(outputAnalyser);
    }
  }, [audioContext, inputNode, outputNode]);

  // Renderizar visualiza√ß√£o em tempo real
  const renderVisualization = useCallback(() => {
    if (!canvasRef.current || !inputAnalyser || !outputAnalyser) return;
    
    const canvas = canvasRef.current;
    const ctx = canvas.getContext('2d');
    if (!ctx) return;
    
    const inputData = new Uint8Array(inputAnalyser.frequencyBinCount);
    const outputData = new Uint8Array(outputAnalyser.frequencyBinCount);
    
    inputAnalyser.getByteFrequencyData(inputData);
    outputAnalyser.getByteFrequencyData(outputData);
    
    // Renderizar visualiza√ß√£o
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    
    // Desenhar barras de frequ√™ncia
    const barWidth = canvas.width / inputData.length;
    for (let i = 0; i < inputData.length; i++) {
      const barHeight = (inputData[i] / 255) * canvas.height;
      ctx.fillStyle = `hsl(${i * 2}, 70%, 50%)`;
      ctx.fillRect(i * barWidth, canvas.height - barHeight, barWidth, barHeight);
    }
    
    requestAnimationFrame(renderVisualization);
  }, [inputAnalyser, outputAnalyser]);

  useEffect(() => {
    const interval = setInterval(renderVisualization, 1000 / 60); // 60 FPS
    return () => clearInterval(interval);
  }, [renderVisualization]);

  return (
    <canvas
      ref={canvasRef}
      width={400}
      height={200}
      className="w-full h-32 bg-gray-900 rounded-lg"
    />
  );
}
```

### **Fase 2: Integra√ß√£o no AnimationSlide**

#### 2.1 Substituir BufferTTSPlayer
```typescript
// components/interactive/AnimationSlide.tsx
import LiveAudioStreamPlayer from '@/components/audio/LiveAudioStreamPlayer';

export default function AnimationSlide({
  title,
  content,
  // ... outros props
}: AnimationSlideProps) {
  return (
    <div className="animation-slide">
      {/* Conte√∫do existente */}
      
      {/* Substituir BufferTTSPlayer por LiveAudioStreamPlayer */}
      {content && (
        <div className="mb-6">
          <LiveAudioStreamPlayer
            text={content}
            voice="Orus"
            autoPlay={false}
            showVisualization={true}
            onAudioStart={() => console.log('√Åudio iniciado')}
            onAudioEnd={() => console.log('√Åudio finalizado')}
            onError={(error) => console.error('Erro de √°udio:', error)}
          />
        </div>
      )}
      
      {/* Resto do conte√∫do */}
    </div>
  );
}
```

#### 2.2 Adicionar Configura√ß√µes de Streaming
```typescript
// Adicionar props para configura√ß√£o de streaming
interface AnimationSlideProps {
  // ... props existentes
  streamingConfig?: {
    enabled: boolean;
    voice: string;
    autoPlay: boolean;
    showVisualization: boolean;
    latency: 'low' | 'medium' | 'high';
  };
}

// Usar configura√ß√£o de streaming
const defaultStreamingConfig = {
  enabled: true,
  voice: 'Orus',
  autoPlay: false,
  showVisualization: true,
  latency: 'low'
};
```

### **Fase 3: Conex√£o Direta com Gemini Live API**

#### 3.1 Criar Hook para Gemini Live
```typescript
// hooks/useGeminiLiveStream.ts
export function useGeminiLiveStream() {
  const [isConnected, setIsConnected] = useState(false);
  const [session, setSession] = useState<Session | null>(null);
  const [isStreaming, setIsStreaming] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const connect = useCallback(async () => {
    try {
      const client = new GoogleGenAI({ 
        apiKey: process.env.NEXT_PUBLIC_GEMINI_API_KEY 
      });
      
      const session = await client.live.connect({
        model: 'gemini-2.5-flash-preview-native-audio-dialog',
        callbacks: {
          onopen: () => {
            setIsConnected(true);
            setError(null);
          },
          onmessage: async (message: LiveServerMessage) => {
            const audio = message.serverContent?.modelTurn?.parts[0]?.inlineData;
            if (audio) {
              // Processar √°udio recebido
              await handleAudioResponse(audio);
            }
          },
          onerror: (e: ErrorEvent) => {
            setError(e.message);
            setIsConnected(false);
          },
          onclose: (e: CloseEvent) => {
            setIsConnected(false);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          speechConfig: {
            voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Orus' } },
          },
        },
      });
      
      setSession(session);
    } catch (error) {
      setError(error instanceof Error ? error.message : 'Erro desconhecido');
    }
  }, []);

  const streamText = useCallback(async (text: string) => {
    if (!session || !text) return;
    
    setIsStreaming(true);
    await session.sendRealtimeInput({ text });
  }, [session]);

  const disconnect = useCallback(() => {
    if (session) {
      session.close();
      setSession(null);
      setIsConnected(false);
    }
  }, [session]);

  return {
    isConnected,
    session,
    isStreaming,
    error,
    connect,
    streamText,
    disconnect
  };
}
```

#### 3.2 Implementar Processamento de √Åudio
```typescript
// utils/audio/liveAudioProcessor.ts
export class LiveAudioProcessor {
  private audioContext: AudioContext;
  private outputNode: AudioNode;
  private nextStartTime: number = 0;
  private sources = new Set<AudioBufferSourceNode>();

  constructor() {
    this.audioContext = new AudioContext({ sampleRate: 24000 });
    this.outputNode = this.audioContext.createGain();
    this.outputNode.connect(this.audioContext.destination);
  }

  async processAudioResponse(audioData: string): Promise<void> {
    try {
      // Decodificar √°udio recebido
      const audioBuffer = await this.decodeAudioData(audioData);
      
      // Criar fonte de √°udio
      const source = this.audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(this.outputNode);
      
      // Configurar timing
      this.nextStartTime = Math.max(
        this.nextStartTime,
        this.audioContext.currentTime
      );
      
      // Reproduzir √°udio
      source.start(this.nextStartTime);
      this.nextStartTime += audioBuffer.duration;
      this.sources.add(source);
      
      // Limpar fonte quando terminar
      source.addEventListener('ended', () => {
        this.sources.delete(source);
      });
      
    } catch (error) {
      console.error('Erro ao processar √°udio:', error);
      throw error;
    }
  }

  private async decodeAudioData(audioData: string): Promise<AudioBuffer> {
    // Implementar decodifica√ß√£o baseada na live-audio
    const binaryString = atob(audioData);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    
    return await this.audioContext.decodeAudioData(bytes.buffer);
  }

  stopAllAudio(): void {
    this.sources.forEach(source => {
      source.stop();
    });
    this.sources.clear();
    this.nextStartTime = 0;
  }
}
```

### **Fase 4: Visualiza√ß√£o Avan√ßada**

#### 4.1 Implementar Visualiza√ß√£o 3D (Opcional)
```typescript
// components/audio/AudioVisualization3D.tsx
import * as THREE from 'three';

export default function AudioVisualization3D({ 
  audioContext, 
  inputNode, 
  outputNode 
}: {
  audioContext: AudioContext;
  inputNode: AudioNode;
  outputNode: AudioNode;
}) {
  const canvasRef = useRef<HTMLCanvasElement>(null);
  const sceneRef = useRef<THREE.Scene | null>(null);
  const rendererRef = useRef<THREE.WebGLRenderer | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);

  useEffect(() => {
    if (!canvasRef.current) return;

    // Configurar Three.js
    const scene = new THREE.Scene();
    const camera = new THREE.PerspectiveCamera(75, 400 / 200, 0.1, 1000);
    const renderer = new THREE.WebGLRenderer({ canvas: canvasRef.current });
    
    // Configurar analisador de √°udio
    const analyser = audioContext.createAnalyser();
    analyser.fftSize = 256;
    outputNode.connect(analyser);
    
    // Criar geometria reativa
    const geometry = new THREE.SphereGeometry(1, 32, 32);
    const material = new THREE.MeshBasicMaterial({ color: 0x00ff00 });
    const sphere = new THREE.Mesh(geometry, material);
    scene.add(sphere);
    
    camera.position.z = 5;
    
    sceneRef.current = scene;
    rendererRef.current = renderer;
    analyserRef.current = analyser;
    
    // Loop de renderiza√ß√£o
    const animate = () => {
      requestAnimationFrame(animate);
      
      if (analyserRef.current) {
        const dataArray = new Uint8Array(analyserRef.current.frequencyBinCount);
        analyserRef.current.getByteFrequencyData(dataArray);
        
        // Animar esfera baseada no √°udio
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        sphere.scale.setScalar(1 + average / 255);
        sphere.rotation.x += 0.01;
        sphere.rotation.y += 0.01;
      }
      
      renderer.render(scene, camera);
    };
    
    animate();
    
    return () => {
      renderer.dispose();
    };
  }, [audioContext, outputNode]);

  return (
    <canvas
      ref={canvasRef}
      width={400}
      height={200}
      className="w-full h-32 bg-gray-900 rounded-lg"
    />
  );
}
```

### **Fase 5: Integra√ß√£o Completa**

#### 5.1 Atualizar DynamicStage
```typescript
// components/interactive/DynamicStage.tsx
import LiveAudioStreamPlayer from '@/components/audio/LiveAudioStreamPlayer';

export default function DynamicStage({ stage, ...props }: DynamicStageProps) {
  const [streamingEnabled, setStreamingEnabled] = useState(true);
  
  return (
    <div className="dynamic-stage">
      {/* Conte√∫do existente */}
      
      {/* Adicionar controles de streaming */}
      <div className="streaming-controls mb-4">
        <label className="flex items-center gap-2">
          <input
            type="checkbox"
            checked={streamingEnabled}
            onChange={(e) => setStreamingEnabled(e.target.checked)}
          />
          <span>Streaming de √°udio em tempo real</span>
        </label>
      </div>
      
      {/* Renderizar atividade com streaming */}
      {renderActivity()}
    </div>
  );
}
```

#### 5.2 Configura√ß√µes Globais
```typescript
// lib/streaming-config.ts
export interface StreamingConfig {
  enabled: boolean;
  voice: string;
  autoPlay: boolean;
  showVisualization: boolean;
  latency: 'low' | 'medium' | 'high';
  fallbackToTTS: boolean;
}

export const defaultStreamingConfig: StreamingConfig = {
  enabled: true,
  voice: 'Orus',
  autoPlay: false,
  showVisualization: true,
  latency: 'low',
  fallbackToTTS: true
};

export function getStreamingConfig(): StreamingConfig {
  if (typeof window === 'undefined') return defaultStreamingConfig;
  
  const saved = localStorage.getItem('streaming-config');
  return saved ? { ...defaultStreamingConfig, ...JSON.parse(saved) } : defaultStreamingConfig;
}

export function saveStreamingConfig(config: StreamingConfig): void {
  if (typeof window === 'undefined') return;
  localStorage.setItem('streaming-config', JSON.stringify(config));
}
```

## üéØ Benef√≠cios da Migra√ß√£o

### **Performance**
- **Lat√™ncia reduzida**: De ~5s para ~100ms
- **Streaming cont√≠nuo**: Reprodu√ß√£o durante gera√ß√£o
- **Qualidade nativa**: PCM sem compress√£o

### **Experi√™ncia do Usu√°rio**
- **Feedback visual**: Visualiza√ß√£o em tempo real
- **Controles intuitivos**: Interface moderna
- **Interatividade**: Resposta imediata

### **Arquitetura**
- **Conex√£o direta**: WebSocket em vez de REST
- **Modularidade**: Componentes reutiliz√°veis
- **Escalabilidade**: Base para futuras funcionalidades

## üìä Compara√ß√£o de Implementa√ß√µes

| Aspecto | Atual (BufferTTS) | Proposta (Live-Audio) |
|---------|-------------------|----------------------|
| **Lat√™ncia** | ~5 segundos | ~100ms |
| **Streaming** | N√£o | Sim |
| **Visualiza√ß√£o** | B√°sica | Avan√ßada |
| **API** | REST | WebSocket direto |
| **Qualidade** | Boa | Nativa |
| **Interatividade** | Limitada | Alta |

## üöÄ Cronograma de Implementa√ß√£o

### **Semana 1: Base**
- [ ] Criar `LiveAudioStreamPlayer`
- [ ] Implementar conex√£o Gemini Live
- [ ] Testes b√°sicos de streaming

### **Semana 2: Integra√ß√£o**
- [ ] Substituir `BufferTTSPlayer` em `AnimationSlide`
- [ ] Implementar visualiza√ß√£o de √°udio
- [ ] Testes de integra√ß√£o

### **Semana 3: Visualiza√ß√£o**
- [ ] Adicionar visualiza√ß√£o 3D (opcional)
- [ ] Implementar controles avan√ßados
- [ ] Otimizar performance

### **Semana 4: Finaliza√ß√£o**
- [ ] Testes completos
- [ ] Documenta√ß√£o
- [ ] Deploy e monitoramento

## üéâ Conclus√£o

A aplica√ß√£o da estrutura de streaming de √°udio da `live-audio` nas aulas resultar√° em:

1. **Experi√™ncia imersiva**: Streaming de √°udio em tempo real
2. **Performance superior**: Lat√™ncia m√≠nima e qualidade nativa
3. **Visualiza√ß√£o avan√ßada**: Feedback visual do √°udio
4. **Arquitetura moderna**: Base s√≥lida para futuras funcionalidades

Esta implementa√ß√£o posicionar√° o sistema de aulas como uma das melhores solu√ß√µes de educa√ß√£o com IA dispon√≠veis, oferecendo uma experi√™ncia de aprendizado verdadeiramente interativa e envolvente.

